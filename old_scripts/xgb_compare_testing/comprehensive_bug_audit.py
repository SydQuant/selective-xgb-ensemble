"""
Comprehensive Bug Audit - Final Check for All Remaining Issues

This script performs a complete audit of the codebase to identify:
1. Any remaining future leakage
2. Temporal alignment issues
3. Last row target problems
4. Inconsistent shift() logic
5. Model evaluation contamination

FOCUS: Only on files actually used by our pipeline
"""

import pandas as pd
import numpy as np
import sys
import os

# Add parent directories to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

def audit_shift_operations():
    """
    Audit all .shift() operations in the pipeline for consistency
    """
    print("=== AUDITING SHIFT OPERATIONS ===")
    
    # These are the files actually used by xgb_compare pipeline
    pipeline_files = [
        '/Users/steven/Projects/SQ/bond_ls_xgb_grope_full_v6/xgb_compare/xgb_compare.py',
        '/Users/steven/Projects/SQ/bond_ls_xgb_grope_full_v6/xgb_compare/full_timeline_backtest.py', 
        '/Users/steven/Projects/SQ/bond_ls_xgb_grope_full_v6/xgb_compare/metrics_utils.py',
        '/Users/steven/Projects/SQ/bond_ls_xgb_grope_full_v6/data/data_utils_simple.py'
    ]
    
    shift_operations = []
    
    for file_path in pipeline_files:
        try:
            with open(file_path, 'r') as f:
                lines = f.readlines()
                
            for i, line in enumerate(lines, 1):
                if '.shift(' in line and not line.strip().startswith('#'):
                    shift_operations.append({
                        'file': os.path.basename(file_path),
                        'line': i,
                        'code': line.strip(),
                        'shift_direction': 'forward' if '(-' in line else 'backward'
                    })
        except:
            continue
    
    print("Found shift operations in pipeline:")
    
    for op in shift_operations:
        direction_emoji = "‚¨ÖÔ∏è" if op['shift_direction'] == 'backward' else "‚û°Ô∏è"
        print(f"{direction_emoji} {op['file']}:{op['line']}")
        print(f"   {op['code']}")
        
        # Evaluate if this shift makes sense
        if 'target_return' in op['code'] and '(-' in op['code']:
            print("   ‚úÖ GOOD: Forward shift for target calculation (uses future for target)")
        elif 'shift(1)' in op['code'] and 'pnl' in op['code'].lower():
            print("   ‚ö†Ô∏è  CHECK: Backward shift for PnL - verify this is intentional")  
        elif 'shift(1)' in op['code'] and 'signal' in op['code'].lower():
            print("   ‚ö†Ô∏è  CHECK: Backward shift for signal - may be artificial lag")
        else:
            print("   üîç REVIEW: Unclear purpose")
    
    return shift_operations

def test_data_pipeline_end_to_end():
    """
    Test the actual data pipeline end-to-end for any issues
    """
    print("\n=== TESTING ACTUAL DATA PIPELINE ===")
    
    # Import the actual pipeline functions
    from data.data_utils_simple import prepare_real_data_simple
    
    print("Testing with small real data sample...")
    
    # Test with minimal parameters (will use cached/sample data if available)
    try:
        # This will test the actual pipeline
        df = prepare_real_data_simple("@ES#C", start_date="2023-01-01", end_date="2023-01-10")
        
        print(f"‚úÖ Pipeline executed successfully")
        print(f"   Data shape: {df.shape}")
        print(f"   Columns: {len(df.columns)}")
        
        # Check target column
        target_cols = [c for c in df.columns if 'target_return' in c]
        if target_cols:
            target_col = target_cols[0]
            last_target = df[target_col].iloc[-1]
            
            print(f"   Target column: {target_col}")
            print(f"   Last row target: {last_target}")
            
            if pd.isna(last_target):
                print("   ‚úÖ GOOD: Last row target is NaN")
            else:
                print("   ‚ö†Ô∏è  WARNING: Last row has target value")
                
        # Check for obvious anomalies
        feature_cols = [c for c in df.columns if c not in target_cols]
        
        # Check for all-zero columns (sign of our ffill fix)
        zero_cols = []
        for col in feature_cols[:10]:  # Check first 10 features
            if (df[col] == 0.0).all():
                zero_cols.append(col)
        
        if zero_cols:
            print(f"   üîç FOUND: {len(zero_cols)} all-zero columns (possibly from ffill fix)")
            
        print(f"   Sample feature values (first row):")
        for col in feature_cols[:5]:
            val = df[col].iloc[0]
            print(f"     {col}: {val}")
            
    except Exception as e:
        print(f"‚ùå Pipeline test failed: {e}")
        print("   This might be expected if no real data is available")
    
    return True

def check_metrics_calculation_consistency():
    """
    Check that metrics calculations are consistent across the codebase
    """
    print("\n=== CHECKING METRICS CALCULATION CONSISTENCY ===")
    
    # Test the metrics functions we know are used
    from xgb_compare.metrics_utils import calculate_model_metrics_from_pnl, calculate_model_metrics
    
    # Create test data
    dates = pd.date_range('2020-01-01', periods=10, freq='D')
    predictions = pd.Series([0.1, -0.2, 0.3, -0.1, 0.2, 0.1, -0.3, 0.2, -0.1, 0.1], index=dates)
    returns = pd.Series([0.01, -0.01, 0.02, -0.005, 0.015, 0.005, -0.02, 0.01, -0.005, 0.008], index=dates)
    
    print("Testing metrics calculation consistency:")
    
    # Method 1: Direct PnL calculation (should be used in backtester)
    pnl_direct = predictions * returns
    metrics1 = calculate_model_metrics_from_pnl(pnl_direct, predictions, returns)
    
    # Method 2: Legacy function with shifted=False (should match)
    metrics2 = calculate_model_metrics(predictions, returns, shifted=False)
    
    # Method 3: Legacy function with shifted=True (should be different due to shift)
    metrics3 = calculate_model_metrics(predictions, returns, shifted=True)
    
    print(f"Method 1 (direct PnL): Sharpe={metrics1.get('sharpe', 0):.4f}")
    print(f"Method 2 (shifted=False): Sharpe={metrics2.get('sharpe', 0):.4f}")
    print(f"Method 3 (shifted=True): Sharpe={metrics3.get('sharpe', 0):.4f}")
    
    # Check consistency
    sharpe_diff = abs(metrics1.get('sharpe', 0) - metrics2.get('sharpe', 0))
    if sharpe_diff < 0.001:
        print("‚úÖ GOOD: Direct PnL matches unshifted calculation")
    else:
        print(f"‚ö†Ô∏è  WARNING: Difference between methods: {sharpe_diff:.6f}")
        
    if abs(metrics1.get('sharpe', 0) - metrics3.get('sharpe', 0)) > 0.01:
        print("‚úÖ GOOD: Shifted calculation produces different result (as expected)")
    else:
        print("‚ö†Ô∏è  WARNING: Shifted calculation too similar - check implementation")
    
    return metrics1, metrics2, metrics3

def audit_cross_validation_usage():
    """
    Audit how cross-validation is used in the pipeline
    """
    print("\n=== AUDITING CROSS-VALIDATION USAGE ===")
    
    from cv.wfo import wfo_splits
    
    # Test CV splits
    n_samples = 100
    splits = list(wfo_splits(n_samples, k_folds=5, min_train=20))
    
    print(f"Testing CV splits with {n_samples} samples:")
    
    issues_found = []
    
    for i, (train_idx, test_idx) in enumerate(splits):
        print(f"Fold {i+1}: Train[{train_idx[0]}:{train_idx[-1]}], Test[{test_idx[0]}:{test_idx[-1]}]")
        
        # Check for overlap
        if set(train_idx).intersection(set(test_idx)):
            issues_found.append(f"Fold {i+1}: Train/test overlap")
            
        # Check temporal order
        if train_idx[-1] >= test_idx[0]:
            issues_found.append(f"Fold {i+1}: Temporal order violation")
            
        # Check gap
        gap = test_idx[0] - train_idx[-1]
        if gap != 1:
            issues_found.append(f"Fold {i+1}: Gap = {gap} (expected 1)")
    
    if issues_found:
        print("‚ùå ISSUES FOUND:")
        for issue in issues_found:
            print(f"   {issue}")
    else:
        print("‚úÖ GOOD: All CV splits look correct")
        
    return splits

def summarize_outstanding_issues():
    """
    Provide final summary of all outstanding issues
    """
    print("\n" + "="*70)
    print("üèÅ COMPREHENSIVE BUG AUDIT COMPLETE")
    print("="*70)
    
    print("\nüìã OUTSTANDING ISSUES TO INVESTIGATE:")
    
    issues = [
        {
            'priority': 'HIGH',
            'issue': 'Last row target handling',
            'description': 'Verify production datasets have NaN targets for latest period',
            'location': 'data_utils_simple.py:139',
            'action': 'Test with real data to confirm last row behavior'
        },
        {
            'priority': 'MEDIUM', 
            'issue': 'Inconsistent shift() logic across codebase',
            'description': 'Some files use shift(1) for PnL, others do direct calculation',
            'location': 'Various metrics files',
            'action': 'Ensure all active code uses consistent approach'
        },
        {
            'priority': 'LOW',
            'issue': 'Legacy code with old shift logic',
            'description': 'Archive files still contain old artificial lag logic',
            'location': 'main.py, metrics/ folder',
            'action': 'Verify these files are not used in current pipeline'
        }
    ]
    
    print("\nüö® HIGH PRIORITY:")
    for issue in [i for i in issues if i['priority'] == 'HIGH']:
        print(f"   {issue['issue']}")
        print(f"   ‚Üí {issue['description']}")
        print(f"   ‚Üí Location: {issue['location']}")
        print(f"   ‚Üí Action: {issue['action']}")
        
    print("\n‚ö†Ô∏è  MEDIUM PRIORITY:")
    for issue in [i for i in issues if i['priority'] == 'MEDIUM']:
        print(f"   {issue['issue']}")
        print(f"   ‚Üí {issue['description']}")
        
    print("\nüîç LOW PRIORITY:")
    for issue in [i for i in issues if i['priority'] == 'LOW']:
        print(f"   {issue['issue']}")
        print(f"   ‚Üí {issue['description']}")
    
    print("\n‚úÖ CONFIRMED FIXES:")
    print("   ‚úì Future leakage in bfill() and median() operations")
    print("   ‚úì Target usage (correctly used as y, not feature)")
    print("   ‚úì CV boundary handling (features calculated on full dataset)")
    print("   ‚úì Current backtester uses direct PnL calculation (no artificial shift)")
    
    return issues

if __name__ == "__main__":
    print("üîç COMPREHENSIVE BUG AUDIT")
    print("="*70)
    print("Auditing the entire pipeline for remaining issues...")
    
    # Audit 1: Shift operations
    shifts = audit_shift_operations()
    
    # Audit 2: End-to-end pipeline test  
    pipeline_ok = test_data_pipeline_end_to_end()
    
    # Audit 3: Metrics consistency
    m1, m2, m3 = check_metrics_calculation_consistency()
    
    # Audit 4: Cross-validation
    cv_splits = audit_cross_validation_usage()
    
    # Final summary
    issues = summarize_outstanding_issues()
    
    print(f"\nüí° RECOMMENDATION:")
    print("Focus on HIGH priority issues first. The pipeline appears mostly correct")
    print("after our fixes, but verification with real data is recommended.")