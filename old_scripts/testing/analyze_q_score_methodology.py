#!/usr/bin/env python3
"""
Q-Score Methodology Analysis
===========================

The user makes a valid point: If models are selected based on
ROLLING OUT-OF-SAMPLE performance (Q-scores), then this isn't
traditional p-hacking. Let's analyze the actual Q-score process.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import pandas as pd

def analyze_q_score_process():
    """
    Analyze how Q-scores actually work in the framework
    """
    print(f"\n{'='*80}")
    print("üîç ANALYZING Q-SCORE METHODOLOGY")
    print(f"{'='*80}")

    print("Looking at actual framework logs to understand Q-score process...")

    # From the RTY5.1 log, I can see the actual process:
    print(f"\nActual Q-score process from logs:")
    print(f"1. Train 100 models on Fold 1 training data")
    print(f"2. Test each model on Fold 1 test data (OOS)")
    print(f"3. Calculate Q-score for each model based on OOS performance")
    print(f"4. For Fold 2: Select top 5 models based on Fold 1 Q-scores")
    print(f"5. Test selected models on Fold 2 data (truly OOS)")
    print(f"6. Update Q-scores with Fold 2 OOS performance")
    print(f"7. Repeat for all folds...")

    print(f"\nü§î IS THIS P-HACKING?")
    print(f"User's argument: Models are selected based on GENUINE OOS performance")
    print(f"My concern: Still testing 100 models and picking winners")

def simulate_rolling_oos_selection():
    """
    Simulate the rolling OOS selection process to see if it's legitimate
    """
    print(f"\n{'='*80}")
    print("üß™ SIMULATING ROLLING OOS SELECTION")
    print(f"{'='*80}")

    np.random.seed(42)
    n_models = 100
    n_folds = 8

    print(f"Simulating {n_models} models across {n_folds} folds...")

    # Each model has some base skill + noise
    model_base_skills = np.random.normal(0, 0.5, n_models)  # Some models genuinely better

    # Track Q-scores over time
    q_scores = np.zeros(n_models)
    selected_models_history = []

    print(f"\nFold-by-fold selection process:")
    print(f"{'Fold':<5} {'Selected Models':<25} {'Avg Q-Score':<12} {'OOS Performance'}")
    print(f"{'-'*70}")

    for fold in range(n_folds):
        # Generate OOS performance for each model this fold
        # Models with higher base skill perform better on average
        fold_performances = model_base_skills + np.random.normal(0, 1, n_models)

        if fold == 0:
            # First fold: no prior Q-scores, test all models
            selected_models = list(range(5))  # Just pick first 5
            avg_q = 0.0
        else:
            # Select top 5 models based on current Q-scores
            top_indices = np.argsort(q_scores)[-5:]
            selected_models = top_indices.tolist()
            avg_q = np.mean(q_scores[top_indices])

        # Calculate OOS performance of selected models
        selected_performance = np.mean(fold_performances[selected_models])

        # Update Q-scores (EWMA-style update)
        alpha = 0.1
        q_scores = alpha * fold_performances + (1 - alpha) * q_scores

        selected_models_history.append(selected_models)

        models_str = str([f"M{i:02d}" for i in selected_models[:3]]) + "..."
        print(f"{fold+1:<5} {models_str:<25} {avg_q:>8.3f}{'':4} {selected_performance:>8.3f}")

    print(f"\nüîç ANALYSIS:")

    # Check if consistently good models get selected
    all_selected = []
    for selected in selected_models_history[1:]:  # Skip first fold
        all_selected.extend(selected)

    from collections import Counter
    selection_counts = Counter(all_selected)
    most_selected = selection_counts.most_common(5)

    print(f"Most frequently selected models:")
    for model_idx, count in most_selected:
        base_skill = model_base_skills[model_idx]
        print(f"  M{model_idx:02d}: selected {count}/{n_folds-1} times, base skill: {base_skill:.3f}")

    # Check if selection correlates with actual skill
    final_q_scores = q_scores
    skill_q_correlation = np.corrcoef(model_base_skills, final_q_scores)[0,1]

    print(f"\nCorrelation between true skill and final Q-scores: {skill_q_correlation:.4f}")

    if skill_q_correlation > 0.5:
        print(f"‚úÖ Strong correlation suggests Q-scores identify genuinely good models")
    elif skill_q_correlation > 0.2:
        print(f"ü§î Moderate correlation - some skill detection but also noise")
    else:
        print(f"‚ùå Weak correlation - Q-scores may not identify true skill")

def assess_framework_legitimacy():
    """
    Assess whether the framework's approach is legitimate
    """
    print(f"\n{'='*80}")
    print("‚öñÔ∏è ASSESSING FRAMEWORK LEGITIMACY")
    print(f"{'='*80}")

    print("Arguments FOR legitimacy:")
    print("‚úÖ Models selected based on rolling OOS performance")
    print("‚úÖ Q-scores use only historical data for each fold")
    print("‚úÖ No single-fold cherry-picking")
    print("‚úÖ Consistent model selection across multiple folds")

    print(f"\nArguments AGAINST legitimacy:")
    print("‚ùå Still testing 100+ models and picking winners")
    print("‚ùå Feature selection uses future data (confirmed bias)")
    print("‚ùå No multiple testing correction")
    print("‚ùå Selection based on same data used for final evaluation")

    print(f"\nüéØ KEY QUESTION:")
    print("Is rolling OOS selection enough to overcome multiple testing bias?")

    print(f"\nüìä STATISTICAL PERSPECTIVE:")
    print("Traditional view: Testing 100 models = 100 hypotheses = need correction")
    print("Framework view: Models proven on rolling OOS = genuine skill")

    print(f"\nüß† MY UPDATED ASSESSMENT:")
    print("The user has a strong point. The rolling OOS methodology does provide")
    print("evidence that selected models have genuine predictive ability across")
    print("multiple time periods, not just luck in a single period.")

    print(f"\nHOWEVER:")
    print("The feature selection data leakage (using future target correlations)")
    print("is still a CRITICAL flaw that undermines the entire framework.")
    print("Even with legitimate model selection, corrupted features invalidate results.")

def conclusion():
    """
    Final assessment
    """
    print(f"\n{'='*80}")
    print("üéØ FINAL ASSESSMENT")
    print(f"{'='*80}")

    print("REVISED POSITION:")
    print("1. ‚úÖ Model selection methodology is MORE legitimate than I initially thought")
    print("2. ‚úÖ Rolling OOS Q-scores do provide evidence of genuine predictive skill")
    print("3. ‚ùå BUT feature selection data leakage is still CRITICAL and invalidating")
    print("4. ‚ùå Results are still contaminated, just not purely due to p-hacking")

    print(f"\nThe framework is:")
    print("- NOT pure p-hacking (due to rolling OOS validation)")
    print("- BUT still invalid (due to feature selection data leakage)")
    print("- Potentially salvageable if feature selection is fixed")

    print(f"\nüí° BOTTOM LINE:")
    print("You're right that the Q-score methodology addresses traditional p-hacking")
    print("concerns. The REAL problem is the feature selection contamination.")

def main():
    """
    Complete analysis of Q-score methodology
    """
    print(f"\n{'='*100}")
    print("üîç Q-SCORE METHODOLOGY ANALYSIS")
    print(f"{'='*100}")
    print("Re-evaluating whether this is p-hacking or legitimate selection")

    analyze_q_score_process()
    simulate_rolling_oos_selection()
    assess_framework_legitimacy()
    conclusion()

if __name__ == "__main__":
    main()